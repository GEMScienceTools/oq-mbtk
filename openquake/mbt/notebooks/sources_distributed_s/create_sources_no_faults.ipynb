{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the distributed seismicity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import re\n",
    "import h5py\n",
    "import numpy\n",
    "import scipy\n",
    "import pickle\n",
    "import matplotlib.pylab as plt\n",
    "from decimal import *\n",
    "getcontext().prec = 4\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from openquake.mbt.oqt_project import OQtProject, OQtSource\n",
    "from openquake.mbt.tools.area import create_catalogue\n",
    "from openquake.mbt.tools.smooth import Smoothing\n",
    "from openquake.mbt.tools.geo import get_idx_points_inside_polygon\n",
    "\n",
    "from openquake.hazardlib.sourcewriter import write_source_model\n",
    "from openquake.hazardlib.source import PointSource, SimpleFaultSource\n",
    "from openquake.hazardlib.mfd.evenly_discretized import EvenlyDiscretizedMFD\n",
    "from openquake.hazardlib.geo.point import Point\n",
    "from openquake.hazardlib.geo.geodetic import azimuth, point_at\n",
    "\n",
    "from openquake.hmtk.seismicity.selector import CatalogueSelector\n",
    "\n",
    "from openquake.hazardlib.scalerel.wc1994 import WC1994\n",
    "from openquake.hazardlib.tom import PoissonTOM\n",
    "from openquake.hazardlib.pmf import PMF\n",
    "from openquake.hazardlib.geo.nodalplane import NodalPlane \n",
    "from openquake.hazardlib.mfd import TruncatedGRMFD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and area source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_pickle_filename = os.environ.get('OQMBT_PROJECT')\n",
    "oqtkp = OQtProject.load_from_file(project_pickle_filename)\n",
    "model_id = oqtkp.active_model_id\n",
    "model = oqtkp.models[model_id]\n",
    "prj_dir = os.path.dirname(project_pickle_filename)\n",
    "print ('Active model ID is:', model_id)\n",
    "area_discretization = model.area_discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_id = getattr(oqtkp,'active_source_id')[0]\n",
    "print ('Area source ID:', src_id)\n",
    "src = model.sources[src_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the nodal plane distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# set the nodal plane distribution\n",
    "nodal_plane_dist_filename = os.path.join(prj_dir, model.nodal_plane_dist_filename)\n",
    "fhdf5 = h5py.File(nodal_plane_dist_filename,'r')\n",
    "#\n",
    "# add the dataset for the current area source, if missing\n",
    "if (src_id in fhdf5.keys() and not ((fhdf5[src_id]['strike'][0] == 0) and \n",
    "                                    (fhdf5[src_id]['dip'][0] == 0) and\n",
    "                                    (fhdf5[src_id]['rake'][0] == 0))):\n",
    "    print ('Using source-specific nodal plane distribution')\n",
    "    data = fhdf5[src_id][:]\n",
    "    tpll = []\n",
    "    for idx in range(0, len(data)):\n",
    "        nplane = NodalPlane(data['strike'][idx],\n",
    "                            data['dip'][idx],\n",
    "                            data['rake'][idx])\n",
    "        tmp = Decimal('{:.2f}'.format(data['wei'][idx]))\n",
    "        tpll.append((Decimal(tmp), nplane))\n",
    "else:\n",
    "    print ('Using default nodal plane distribution')\n",
    "    tpll = []\n",
    "    npd = model.default_nodal_plane_dist\n",
    "    for idx in range(0, len(npd['strike'])):\n",
    "        nplane = NodalPlane(npd['strike'][idx],\n",
    "                            npd['dip'][idx],\n",
    "                            npd['rake'][idx])\n",
    "        tmp = Decimal('{:.2f}'.format(data['wei'][idx]))\n",
    "        tpll.append((Decimal(tmp), nplane))\n",
    "nodal_plane_distribution = PMF(tpll)    \n",
    "fhdf5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the hypocentral depth distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# read hypocentral depth file\n",
    "hypo_dist_filename = os.path.join(prj_dir, model.hypo_dist_filename)\n",
    "fhdf5 = h5py.File(hypo_dist_filename,'r')\n",
    "#\n",
    "# check if the file contains information relative this source\n",
    "if (src_id in fhdf5.keys() and not ((fhdf5[src_id]['depth'][0] == 0) and \n",
    "                                    (fhdf5[src_id]['wei'][0] == 0))):\n",
    "    print('Using source-specific hypocentral depth distribution')\n",
    "    data = fhdf5[src_id][:]\n",
    "    tpll = []\n",
    "    for idx in range(0, len(data)):\n",
    "        tmp = Decimal('{:.2f}'.format(data['wei'][idx]))\n",
    "        tpll.append((Decimal(tmp), data['depth'][idx]))\n",
    "else:\n",
    "    print('Using default hypocentral depth distribution')\n",
    "    tpll = []\n",
    "    hdd = model.default_hypo_dist\n",
    "    for idx in range(0, len(hdd['dep'])):\n",
    "        tmp = Decimal('{:.2f}'.format(data['wei'][idx]))\n",
    "        tpll.append((Decimal(tmp), data['depth'][idx]))\n",
    "hypocenter_distribution = PMF(tpll)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dilated polygon around the area source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_polygon = src.polygon.dilate(100)\n",
    "polygon_mesh = new_polygon.discretize(area_discretization)\n",
    "print ('Number of points: %d' % (len(polygon_mesh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# get the earthquakes of the catalogue within the dilated polygon \n",
    "pickle_filename = os.path.join(prj_dir, oqtkp.models[model_id].declustered_catalogue_pickle_filename)\n",
    "fin = open(pickle_filename, 'rb') \n",
    "catalogue = pickle.load(fin)\n",
    "fin.close()\n",
    "print ('The calogue contains %d earthquakes' % (len(catalogue.data['magnitude'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# create the subcatalogue for the dilated polygon\n",
    "cutoff_magnitude = float(model.catalogue_cutoff_magnitude)\n",
    "fcatal = create_catalogue(model, catalogue, polygon=new_polygon)\n",
    "selector = CatalogueSelector(catalogue, create_copy=False)\n",
    "tmp = selector.within_magnitude_range(cutoff_magnitude, 10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_param = model.smoothing_param\n",
    "smooth = Smoothing(fcatal, polygon_mesh, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = smooth.multiple_smoothing(smooth_param)\n",
    "print ('Max smoothing value:', max(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map with the smoothed grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "tmp = plt.scatter(smooth.mesh.lons, smooth.mesh.lats, c=values, vmin=0, vmax=max(values), marker='s', s=15)\n",
    "tmp = plt.plot(src.polygon.lons, src.polygon.lats, 'r')\n",
    "tmp = plt.plot(fcatal.data['longitude'], fcatal.data['latitude'], 'og', mfc='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm tmp*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the nodes of the grid within the area source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxp = smooth.get_points_in_polygon(src.polygon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.scatter(smooth.mesh.lons[idxp], smooth.mesh.lats[idxp], vmin=0, vmax=0.4, c=values[idxp], marker='s', s=15)\n",
    "plt.plot(src.polygon.lons, src.polygon.lats, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign seismicity to the source\n",
    "The redistribution of seismicity to the source is done for each cell using as a scaling factor the ratio of the value assigned to the node and the sum of the values of all the nodes within the area source. Note that the mfd assigned to the area source must be an EvenlyDiscretisedMFD instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_factor = numpy.zeros((len(values)))\n",
    "sumwei = sum(values[idxp])\n",
    "for i in idxp:\n",
    "    scaling_factor[i] = values[i]/sumwei\n",
    "assert(numpy.abs(sum(scaling_factor[idxp]) - 1.0) < 1e-2) \n",
    "#\n",
    "# if necessary convert a TruncatedGR into a EvenlyDiscretizedMFD \n",
    "if isinstance(src.mfd, TruncatedGRMFD):  \n",
    "    tmp = src.mfd.get_annual_occurrence_rates()\n",
    "    rates = [r[1] for r in tmp]\n",
    "    mfd = EvenlyDiscretizedMFD(tmp[0][0], tmp[1][0] - tmp[0][0], rates) \n",
    "else:\n",
    "    mfd = source.mfd\n",
    "#\n",
    "# compute scaled rates\n",
    "mfdpnts = numpy.array([mfd.occurrence_rates,]*len(values))\n",
    "for i in idxp:\n",
    "    mfdpnts[i, :] = mfdpnts[i, :] * scaling_factor[i]\n",
    "#\n",
    "#\n",
    "#xxx = numpy.tile(values, (mfdpnts.shape[1], 1)).T\n",
    "#mfdpnts = mfdpnts * numpy.tile(values, (mfdpnts.shape[1], 1)).T\n",
    "#\n",
    "# create a vector of magnitude values\n",
    "mags = []\n",
    "for mag, _ in mfd.get_annual_occurrence_rates():\n",
    "    mags.append(mag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the nrml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openquake.mbt.tools.mfd import EEvenlyDiscretizedMFD\n",
    "\n",
    "model.upper_seismogenic_depth = float(model.upper_seismogenic_depth)\n",
    "model.lower_seismogenic_depth = float(model.lower_seismogenic_depth)\n",
    "# info\n",
    "print (\"model upper seismogenic depth: {:.2f}\".format(model.upper_seismogenic_depth))\n",
    "print (\"model lower seismogenic depth: {:.2f}\".format(model.lower_seismogenic_depth))\n",
    "nrmls = [] \n",
    "first = True\n",
    "#\n",
    "# settings\n",
    "rupture_mesh_spacing = 2.5\n",
    "magnitude_scaling_relationship = WC1994()\n",
    "\n",
    "rupture_aspect_ratio = 2.0\n",
    "temporal_occurrence_model = PoissonTOM(1.)\n",
    "#\n",
    "# loop on the points inside the polygon\n",
    "for eee, iii in enumerate(idxp):\n",
    "    #\n",
    "    # check if the discrete MFD for this point has values larger than 0\n",
    "    jjj = numpy.nonzero(mfdpnts[iii, :] > 0)\n",
    "    if len(list(mfdpnts[iii, jjj][0])) > 0:\n",
    "        #\n",
    "        # create the MFD\n",
    "        tmfd = EvenlyDiscretizedMFD(src.mfd.min_mag+src.mfd.bin_width/2, src.mfd.bin_width, list(mfdpnts[iii, jjj][0]))\n",
    "        \n",
    "        if first:\n",
    "            totmfd = EEvenlyDiscretizedMFD.from_mfd(tmfd)\n",
    "            first = False\n",
    "        else:\n",
    "            totmfd.stack(EEvenlyDiscretizedMFD.from_mfd(tmfd))\n",
    "            \n",
    "        #\n",
    "        # create the point source\n",
    "        points = PointSource(\n",
    "            source_id='{:s}_{:d}'.format(src.source_id, eee), \n",
    "            name='', \n",
    "            tectonic_region_type=src.tectonic_region_type,\n",
    "            mfd=tmfd, \n",
    "            rupture_mesh_spacing=rupture_mesh_spacing,\n",
    "            magnitude_scaling_relationship=magnitude_scaling_relationship, \n",
    "            rupture_aspect_ratio=rupture_aspect_ratio,\n",
    "            temporal_occurrence_model=temporal_occurrence_model,\n",
    "            upper_seismogenic_depth=model.upper_seismogenic_depth, \n",
    "            lower_seismogenic_depth=model.lower_seismogenic_depth,\n",
    "            location=Point(smooth.mesh.lons[iii], smooth.mesh.lats[iii]), \n",
    "            nodal_plane_distribution=nodal_plane_distribution, \n",
    "            hypocenter_distribution=hypocenter_distribution\n",
    "            )\n",
    "        nrmls.append(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# write the nrml file\n",
    "model_dir = os.path.join(prj_dir, 'nrml/%s' % (re.sub('\\s','_',model_id)))\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "model_name = '%s_gridded_seismicity_source_%s.xml' % (model_id, src_id)\n",
    "out_model_name = os.path.join(model_dir, model_name)\n",
    "_ = write_source_model(out_model_name, nrmls, 'Model {:s}'.format(model.name))\n",
    "print('Created %s ' % (out_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "occ = numpy.array(src.mfd.get_annual_occurrence_rates())\n",
    "plt.plot(occ[:,0], occ[:,1], label='original')\n",
    "occ = numpy.array(totmfd.get_annual_occurrence_rates())\n",
    "plt.plot(occ[:,0], occ[:,1], ':r', label='stacked')\n",
    "plt.yscale('log')\n",
    "_ = plt.legend(fontsize=14)\n",
    "plt.xlabel('Magnitude', fontsize=14)\n",
    "plt.ylabel('Occurrence rate', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "none",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
